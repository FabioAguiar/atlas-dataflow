# üìÑ evaluate.model_selection ‚Äî Sele√ß√£o de Modelo Campe√£o (v1)

## Vis√£o Geral

Esta spec define o Step **evaluate.model_selection v1** do **Atlas DataFlow**, respons√°vel por
**selecionar explicitamente o modelo campe√£o** ap√≥s a avalia√ß√£o padronizada de m√©tricas.

No Atlas, a decis√£o de promo√ß√£o de modelo √© **um ato expl√≠cito e audit√°vel**, nunca um efeito
colateral do treino, da busca ou de qualquer outro Step.

---

## Objetivo

- Selecionar o modelo campe√£o com base em **m√©trica alvo configur√°vel**
- Registrar crit√©rio, ranking e decis√£o de forma **serializ√°vel e est√°vel**
- Garantir decis√£o **determin√≠stica e reprodut√≠vel** para inputs fixos

---

## Natureza do Step

- **ID:** `evaluate.model_selection`
- **Kind:** `evaluate`
- **Milestone:** M5 ‚Äî Modelagem & Avalia√ß√£o
- **Car√°ter:** Decis√≥rio (n√£o treina nem recalcula m√©tricas)

---

## Depend√™ncias

O Step depende semanticamente de:

- `evaluate.metrics`
- M√©tricas compar√°veis (mesmo split/dataset de avalia√ß√£o)
- Conjunto de candidatos (um por modelo) dispon√≠vel no RunContext

---

## Configura√ß√£o Esperada

```yaml
steps:
  evaluate.model_selection:
    enabled: true
    target_metric: f1
    direction: maximize   # maximize | minimize
```

Campos:
- `enabled` (opcional, default: true)
- `target_metric` (obrigat√≥rio)
- `direction` (obrigat√≥rio: `maximize` | `minimize`)

---

## Entradas (Artifacts esperados)

O Step **n√£o faz descoberta autom√°tica** de fontes de m√©tricas. Ele suporta somente
artifacts expl√≠citos, com formatos est√°veis:

### A) `eval.metrics` como lista (recomendado)
Artifact: `eval.metrics`

```yaml
- model_id: logistic_regression
  metrics:
    f1: 0.81
    accuracy: 0.79
- model_id: random_forest
  metrics:
    f1: 0.83
    accuracy: 0.78
```

### B) `eval.metrics` como dict (caso unit√°rio)
Artifact: `eval.metrics`

```yaml
model_id: logistic_regression
metrics:
  f1: 0.81
  accuracy: 0.79
```

### C) `eval.metrics_list` como lista (compat / alternativa expl√≠cita)
Artifact: `eval.metrics_list`  
Mesmo formato do item A.

> Regras de valida√ß√£o:
> - `model_id` √© obrigat√≥rio em cada candidato (n√£o-infer√™ncia).
> - `metrics` deve ser um mapping/dict.
> - `target_metric` deve existir em **todos** os candidatos.

---

## Comportamento Can√¥nico

O Step deve:

1. Ler `target_metric` e `direction` da configura√ß√£o
2. Carregar a lista de candidatos a partir de `eval.metrics_list` **ou** `eval.metrics`
3. Validar:
   - exist√™ncia de `model_id`
   - presen√ßa de `metrics[target_metric]`
4. Produzir um **ranking** ordenado conforme `direction`
5. Resolver empates de forma **determin√≠stica**
6. Selecionar o primeiro do ranking como **campe√£o**
7. Persistir o payload final no artifact `eval.model_selection`
8. Registrar a decis√£o (crit√©rio + campe√£o) de forma rastre√°vel no runtime/manifest

---

## Payload Esperado (m√≠nimo)

Artifact gerado: `eval.model_selection`

```yaml
payload:
  selection:
    metric: string
    direction: maximize | minimize
    champion_model_id: string
    champion_score: float
    ranking:
      - model_id: string
        score: float
```

Regras:
- Payload **100% serializ√°vel**
- `ranking` deve ser **determin√≠stico** e **est√°vel**
- Nada √© inferido automaticamente

---

## Regras de Determinismo (desempate)

Empates s√£o resolvidos por regra expl√≠cita e est√°vel:

1. Ordena√ß√£o prim√°ria por `score`:
   - `maximize`: maior score primeiro
   - `minimize`: menor score primeiro
2. Em caso de empate, ordenar por `model_id` em ordem **lexicogr√°fica crescente**

Assim:
- Mesmos inputs ‚Üí mesmo campe√£o
- Decis√£o reproduz√≠vel em execu√ß√µes distintas

---

## Falhas Expl√≠citas

O Step deve falhar quando:

- nenhum artifact de m√©tricas existir (`eval.metrics` ou `eval.metrics_list`)
- lista de candidatos estiver vazia
- `target_metric` n√£o existir para algum candidato
- `direction` inv√°lida
- `metrics` n√£o for dict / score n√£o for num√©rico

---

## Testes Esperados

Os testes unit√°rios devem cobrir:

- sele√ß√£o correta do campe√£o (`maximize` e `minimize`)
- respeito √† m√©trica alvo
- desempate determin√≠stico
- suporte a `eval.metrics` como lista e como dict
- falha expl√≠cita para configura√ß√µes inv√°lidas e inputs ausentes
- comportamento **skip** quando `enabled: false`

---

## Fora de Escopo (v1)

- Sele√ß√£o multiobjetivo / Pareto
- Ensemble autom√°tico
- Visualiza√ß√µes / dashboards
- Persist√™ncia de modelos ou artefatos de treino (isso pertence a outras issues)

---

## Evolu√ß√£o Futura

Poss√≠veis extens√µes:

- sele√ß√£o multi-m√©trica (ex.: regra ponderada)
- crit√©rios customizados (por dom√≠nio)
- integra√ß√£o com leaderboard persistente
- persist√™ncia do ‚Äúcampe√£o‚Äù como artefato versionado do run

---

## Refer√™ncias

- `docs/spec/evaluate.metrics.v1.md`
- `docs/spec/train.single.v1.md`
- `docs/spec/train.search.v1.md`
- `docs/pipeline_elements.md`
- `docs/engine.md`
- `docs/traceability.md`
- `docs/manifest.schema.v1.md`
- `docs/testing.md`
